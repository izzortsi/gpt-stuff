{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from text_generation import generate, complete\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPEN_AI_API_KEY\")\n",
    "openai.api_base = 'https://api.openai.com/v1'\n",
    "MODEL = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "# openai.api_key = os.environ.get(\"OPEN_AI_FREE_API_KEY\")\n",
    "# openai.api_base = 'https://api.pawan.krd/v1'\n",
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "#You will have to organize all this information and help me make sense of it.\n",
    "SYS_PROMPT = \"\"\"You are a smart text expander. Your goal is to expand poorly given instructions for tasks into a list of instructions that achieves the objective implied by the poorly given instructions.\n",
    "                I will give you some examples of the instructions that you will be given, each example is delimited by triple backticks. Here they are:\n",
    "                \n",
    "                ```so tasks\n",
    "\n",
    "                1. run that script and check output (it doesnt have to run to completion)\n",
    "\n",
    "                2. modify stable diffusion library so we can feed in noise vector by hand\n",
    "\n",
    "                3. look at the model.py thing\n",
    "                - see if we can make a file to load the unet, the text encoder and the auto-encoder as seperate/independent classes\n",
    "                - so we can load one and use only one without running whole model```\n",
    "\n",
    "                ```1. revert changes\n",
    "                 2. make a new class\n",
    "                 - one class, in one file, for each of the models\n",
    "                 - encode/decode for auto-encoder, etc\n",
    " \n",
    "                 3. function for load, unload\n",
    " \n",
    "                 4. function for batched vs single input\n",
    " \n",
    "                 5. In Scripts\n",
    "                 - make a new example\n",
    "                 - where you load the 3 classes\n",
    "                 - then encode the prompt\n",
    "                 - then run denoising 20 times\n",
    "                 - then output latent to image\n",
    "                 -- function to save image\n",
    " \n",
    "                 6. Make sure the inner loop, we can feed in noise vector directly\n",
    " \n",
    "                 But dont change the old pipeline files yet```\n",
    "\n",
    "                ```in the main /existing work flow; you use all three\n",
    "                - unet\n",
    "                - denoiser\n",
    "                - clip\n",
    "\n",
    "                in one run\n",
    "\n",
    "                But in the alternative work flow\n",
    "\n",
    "                - you load clip\n",
    "                -- run 1024 times\n",
    "                - unload clip\n",
    "\n",
    "                - you load denoiser\n",
    "                -- you run 1024 times\n",
    "                - unload denoise\n",
    "\n",
    "                - you load encoder\n",
    "                -- you run 1024 times\n",
    "                - unload encoder\n",
    "\n",
    "\n",
    "                So its \"Batched\" so you are not switching between which network is used during runtime```\n",
    "\n",
    "\n",
    "                ```Ticket:\n",
    "\n",
    "                1. copy the files over\n",
    "                - duplicate it\n",
    "\n",
    "                2. Make function\n",
    "                - loads whole model with first copy\n",
    "\n",
    "                3. Make function\n",
    "                - saves 3 models\n",
    "                -- embedding\n",
    "                -- unet\n",
    "                -- variational auto encoder\n",
    "\n",
    "                as seperate files to output\n",
    "\n",
    "                4. Load those files with second model\n",
    "                - and track the tile\n",
    "                - each model is a seperate file\n",
    "                -- model_clip.y\n",
    "                -- model_unet.py\n",
    "                -- model_vae.py\n",
    "\n",
    "                Each model class must have a Load and Unload function. No loading model on init.\n",
    "\n",
    "                5. Use safe tensor, not .cpt; it might be faster and is the newer format\n",
    "\n",
    "                6. Do test to make sure we are getting same result```\n",
    "\n",
    "                Learn how to expand each of the examples given.\n",
    "                Using this knowledge, you will expand the instructions I will give you in future messages. \n",
    "                Reply my messages strictly with expansions for the instructions I will give you. Do not reply with anything else.\n",
    "                I will give you instructions delimited by triple backticks, as in the examples.\n",
    "                \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, sys_prompt=SYS_PROMPT, model=MODEL, temperature = TEMPERATURE):\n",
    "        self._sys_messages = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "        self._messages = self._sys_messages\n",
    "        self.response = \"\"\n",
    "        self._model = model\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def set_system(self, sys_prompt):\n",
    "        self._sys_messages = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "    \n",
    "    def add_system(self, sys_prompt):\n",
    "        self._sys_messages.append({\"role\": \"system\", \"content\": sys_prompt})\n",
    "\n",
    "    def completion(self, prompt, role = \"user\", chat=False):\n",
    "        user_message = [{\"role\": role, \"content\": prompt}]\n",
    "        self._messages += user_message\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self._model,\n",
    "            messages=self._messages,\n",
    "            temperature=self._temperature, # this is the degree of randomness of the model's output\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        # self.response = response\n",
    "        self.response = response.choices[0].message[\"content\"]\n",
    "        # print(self.response)\n",
    "        self._messages += [{\"role\": \"assistant\", \"content\": self.response}]\n",
    "            \n",
    "        return self.response\n",
    "\n",
    "def chat(gpt):\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt == \"exit\":\n",
    "            break\n",
    "\n",
    "        print(\"Bot:\", gpt.completion(prompt, chat=True))\n",
    "\n",
    "GPT.chat = chat\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Duplicate the files by copying them over.\\n\\n2. Create a function that loads the whole model using the first copy.\\n\\n3. Create a function that saves three models as separate files: embedding, unet, and variational auto encoder.\\n\\n4. Load the files for the second model and track the tile. Each model should be saved as a separate file: model_clip.y, model_unet.py, and model_vae.py.\\n\\n5. Ensure that each model class has a Load and Unload function. Do not load the model upon initialization.\\n\\n6. Use the safe tensor format instead of .cpt, as it might be faster and is the newer format.\\n\\n7. Perform a test to ensure that the results are the same.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Ticket:\n",
    "\n",
    "1. copy the files over\n",
    "- duplicate it\n",
    "\n",
    "2. Make function\n",
    "- loads whole model with first copy\n",
    "\n",
    "3. Make function\n",
    "- saves 3 models\n",
    "-- embedding\n",
    "-- unet\n",
    "-- variational auto encoder\n",
    "\n",
    "as seperate files to output\n",
    "\n",
    "4. Load those files with second model\n",
    "- and track the tile\n",
    "- each model is a seperate file\n",
    "-- model_clip.y\n",
    "-- model_unet.py\n",
    "-- model_vae.py\n",
    "\n",
    "Each model class must have a Load and Unload function. No loading model on init.\n",
    "\n",
    "5. Use safe tensor, not .cpt; it might be faster and is the newer format\n",
    "\n",
    "6. Do test to make sure we are getting same result\n",
    "\"\"\"\n",
    "gpt.completion(prompt= prompt, role = \"user\", chat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Duplicate the files by copying them over.\n",
      "\n",
      "2. Create a function that loads the whole model using the first copy.\n",
      "\n",
      "3. Create a function that saves three models as separate files: embedding, unet, and variational auto encoder.\n",
      "\n",
      "4. Load the files for the second model and track the tile. Each model should be saved as a separate file: model_clip.y, model_unet.py, and model_vae.py.\n",
      "\n",
      "5. Ensure that each model class has a Load and Unload function. Do not load the model upon initialization.\n",
      "\n",
      "6. Use the safe tensor format instead of .cpt, as it might be faster and is the newer format.\n",
      "\n",
      "7. Perform a test to ensure that the results are the same.\n"
     ]
    }
   ],
   "source": [
    "print(gpt.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
