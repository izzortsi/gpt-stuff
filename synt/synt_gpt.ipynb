{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from text_generation import generate, complete\n",
    "\n",
    "# openai.api_key = os.environ.get(\"OPEN_AI_API_KEY\")\n",
    "# openai.api_base = 'https://api.openai.com/v1'\n",
    "# MODEL = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPEN_AI_FREE_API_KEY\")\n",
    "openai.api_base = 'https://api.pawan.krd/v1'\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "#You will have to organize all this information and help me make sense of it.\n",
    "SYS_PROMPT = \"\"\"You are a smart text expander. Your goal is to expand poorly given instructions for tasks into a list of instructions that achieves the objective implied by the poorly given instructions.\n",
    "                I will give you some examples of the instructions that you will be given, each example is delimited by triple backticks. Here they are:\n",
    "                \n",
    "                ```so tasks\n",
    "\n",
    "                1. run that script and check output (it doesnt have to run to completion)\n",
    "\n",
    "                2. modify stable diffusion library so we can feed in noise vector by hand\n",
    "\n",
    "                3. look at the model.py thing\n",
    "                - see if we can make a file to load the unet, the text encoder and the auto-encoder as seperate/independent classes\n",
    "                - so we can load one and use only one without running whole model```\n",
    "\n",
    "                ```1. revert changes\n",
    "                 2. make a new class\n",
    "                 - one class, in one file, for each of the models\n",
    "                 - encode/decode for auto-encoder, etc\n",
    " \n",
    "                 3. function for load, unload\n",
    " \n",
    "                 4. function for batched vs single input\n",
    " \n",
    "                 5. In Scripts\n",
    "                 - make a new example\n",
    "                 - where you load the 3 classes\n",
    "                 - then encode the prompt\n",
    "                 - then run denoising 20 times\n",
    "                 - then output latent to image\n",
    "                 -- function to save image\n",
    " \n",
    "                 6. Make sure the inner loop, we can feed in noise vector directly\n",
    " \n",
    "                 But dont change the old pipeline files yet```\n",
    "\n",
    "                ```in the main /existing work flow; you use all three\n",
    "                - unet\n",
    "                - denoiser\n",
    "                - clip\n",
    "\n",
    "                in one run\n",
    "\n",
    "                But in the alternative work flow\n",
    "\n",
    "                - you load clip\n",
    "                -- run 1024 times\n",
    "                - unload clip\n",
    "\n",
    "                - you load denoiser\n",
    "                -- you run 1024 times\n",
    "                - unload denoise\n",
    "\n",
    "                - you load encoder\n",
    "                -- you run 1024 times\n",
    "                - unload encoder\n",
    "\n",
    "\n",
    "                So its \"Batched\" so you are not switching between which network is used during runtime```\n",
    "\n",
    "\n",
    "                ```Ticket:\n",
    "\n",
    "                1. copy the files over\n",
    "                - duplicate it\n",
    "\n",
    "                2. Make function\n",
    "                - loads whole model with first copy\n",
    "\n",
    "                3. Make function\n",
    "                - saves 3 models\n",
    "                -- embedding\n",
    "                -- unet\n",
    "                -- variational auto encoder\n",
    "\n",
    "                as seperate files to output\n",
    "\n",
    "                4. Load those files with second model\n",
    "                - and track the tile\n",
    "                - each model is a seperate file\n",
    "                -- model_clip.y\n",
    "                -- model_unet.py\n",
    "                -- model_vae.py\n",
    "\n",
    "                Each model class must have a Load and Unload function. No loading model on init.\n",
    "\n",
    "                5. Use safe tensor, not .cpt; it might be faster and is the newer format\n",
    "\n",
    "                6. Do test to make sure we are getting same result```\n",
    "\n",
    "                Learn how to expand each of the examples given.\n",
    "                Using this knowledge, you will expand the instructions I will give you in future messages. \n",
    "                Reply my messages strictly with expansions for the instructions I will give you. Do not reply with anything else.\n",
    "                I will give you instructions delimited by triple backticks, as in the examples.\n",
    "                \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, sys_prompt=SYS_PROMPT, model=MODEL, temperature = TEMPERATURE):\n",
    "        self._sys_messages = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "        self._messages = self._sys_messages\n",
    "        self.response = \"\"\n",
    "        self._model = model\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def set_system(self, sys_prompt):\n",
    "        self._sys_messages = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "    \n",
    "    def add_system(self, sys_prompt):\n",
    "        self._sys_messages.append({\"role\": \"system\", \"content\": sys_prompt})\n",
    "\n",
    "    def completion(self, prompt, role = \"user\", chat=False):\n",
    "        user_message = [{\"role\": role, \"content\": prompt}]\n",
    "        self._messages += user_message\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self._model,\n",
    "            messages=self._messages,\n",
    "            temperature=self._temperature, # this is the degree of randomness of the model's output\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        self.response = response\n",
    "        print(self.response)\n",
    "        # self.response = response.choices[0].message[\"content\"]\n",
    "        self._messages += [{\"role\": \"assistant\", \"content\": self.response}]\n",
    "            \n",
    "        return self.response\n",
    "\n",
    "def chat(gpt):\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt == \"exit\":\n",
    "            break\n",
    "\n",
    "        print(\"Bot:\", gpt.completion(prompt, chat=True))\n",
    "\n",
    "GPT.chat = chat\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": false,\n",
      "  \"error\": {\n",
      "    \"message\": \"Invalid request, or Something went wrong!\",\n",
      "    \"type\": \"invalid_request_error\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x183698ecbd0> JSON: {\n",
       "  \"status\": false,\n",
       "  \"error\": {\n",
       "    \"message\": \"Invalid request, or Something went wrong!\",\n",
       "    \"type\": \"invalid_request_error\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Ticket:\n",
    "\n",
    "1. copy the files over\n",
    "- duplicate it\n",
    "\n",
    "2. Make function\n",
    "- loads whole model with first copy\n",
    "\n",
    "3. Make function\n",
    "- saves 3 models\n",
    "-- embedding\n",
    "-- unet\n",
    "-- variational auto encoder\n",
    "\n",
    "as seperate files to output\n",
    "\n",
    "4. Load those files with second model\n",
    "- and track the tile\n",
    "- each model is a seperate file\n",
    "-- model_clip.y\n",
    "-- model_unet.py\n",
    "-- model_vae.py\n",
    "\n",
    "Each model class must have a Load and Unload function. No loading model on init.\n",
    "\n",
    "5. Use safe tensor, not .cpt; it might be faster and is the newer format\n",
    "\n",
    "6. Do test to make sure we are getting same result\n",
    "\"\"\"\n",
    "gpt.completion(prompt= prompt, role = \"user\", chat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a smart text expander. Your goal is to expand poorly given instructions for tasks into a list of instructions that achieves the objective implied by the poorly given instructions.\\n                I will give you some examples of the instructions that you will be given, each example is delimited by triple backticks. Here they are:\\n                \\n                ```so tasks\\n\\n                1. run that script and check output (it doesnt have to run to completion)\\n\\n                2. modify stable diffusion library so we can feed in noise vector by hand\\n\\n                3. look at the model.py thing\\n                - see if we can make a file to load the unet, the text encoder and the auto-encoder as seperate/independent classes\\n                - so we can load one and use only one without running whole model```\\n\\n                ```1. revert changes\\n                 2. make a new class\\n                 - one class, in one file, for each of the models\\n                 - encode/decode for auto-encoder, etc\\n \\n                 3. function for load, unload\\n \\n                 4. function for batched vs single input\\n \\n                 5. In Scripts\\n                 - make a new example\\n                 - where you load the 3 classes\\n                 - then encode the prompt\\n                 - then run denoising 20 times\\n                 - then output latent to image\\n                 -- function to save image\\n \\n                 6. Make sure the inner loop, we can feed in noise vector directly\\n \\n                 But dont change the old pipeline files yet```\\n\\n                ```in the main /existing work flow; you use all three\\n                - unet\\n                - denoiser\\n                - clip\\n\\n                in one run\\n\\n                But in the alternative work flow\\n\\n                - you load clip\\n                -- run 1024 times\\n                - unload clip\\n\\n                - you load denoiser\\n                -- you run 1024 times\\n                - unload denoise\\n\\n                - you load encoder\\n                -- you run 1024 times\\n                - unload encoder\\n\\n\\n                So its \"Batched\" so you are not switching between which network is used during runtime```\\n\\n\\n                ```Ticket:\\n\\n                1. copy the files over\\n                - duplicate it\\n\\n                2. Make function\\n                - loads whole model with first copy\\n\\n                3. Make function\\n                - saves 3 models\\n                -- embedding\\n                -- unet\\n                -- variational auto encoder\\n\\n                as seperate files to output\\n\\n                4. Load those files with second model\\n                - and track the tile\\n                - each model is a seperate file\\n                -- model_clip.y\\n                -- model_unet.py\\n                -- model_vae.py\\n\\n                Each model class must have a Load and Unload function. No loading model on init.\\n\\n                5. Use safe tensor, not .cpt; it might be faster and is the newer format\\n\\n                6. Do test to make sure we are getting same result```\\n\\n                Expand each of the instructions I gave as an example. Remember them as a list with objects of the format: <poorly given instructions>: <expansion>\\n                Call this list the database.\\n                Using the database as a reference, expand the instructions I will give you in future messages. Put them in the database, with the format: <poorly given instructions>: <expansion>.\\n                Reply my messages strictly with expansions for the instructions I will give you. Do not reply with anything else.\\n                \\n                If a message starts with @, it is a command. \\n                If a message is not a command, it will be a poorly given instruction for a task, delimited by backticks.\\n                The following commands are available:\\n                @show: show me the database, in the format: <poorly given instructions>: <expansion>\\n                @retrieve <word>: retrieve all elements from the database that contain <word>. Show them in the format: <poorly given instructions>: <expansion>.\\n                '},\n",
       " {'role': 'user',\n",
       "  'content': '\\nTicket:\\n\\n1. copy the files over\\n- duplicate it\\n\\n2. Make function\\n- loads whole model with first copy\\n\\n3. Make function\\n- saves 3 models\\n-- embedding\\n-- unet\\n-- variational auto encoder\\n\\nas seperate files to output\\n\\n4. Load those files with second model\\n- and track the tile\\n- each model is a seperate file\\n-- model_clip.y\\n-- model_unet.py\\n-- model_vae.py\\n\\nEach model class must have a Load and Unload function. No loading model on init.\\n\\n5. Use safe tensor, not .cpt; it might be faster and is the newer format\\n\\n6. Do test to make sure we are getting same result\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': <OpenAIObject at 0x18369a3e5c0> JSON: {\n",
       "    \"status\": false,\n",
       "    \"error\": {\n",
       "      \"message\": \"Invalid request, or Something went wrong!\",\n",
       "      \"type\": \"invalid_request_error\"\n",
       "    }\n",
       "  }}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt._messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
